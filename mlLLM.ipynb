{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "669bc485-717c-4d11-8f55-58ee2416aa16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTRODUCTION TO  MACHINE LEARNING AN EARLY DRAFT OF A PROPOSED TEXTBOOK  Nils J. Nilsson Robotics Laboratory Department of Computer Science Stanford University Stanford, CA 94305 e-mail: nilsson@cs.stanford.edu November 3, 1998  Copyright c 2005 Nils J. Nilsson This material may not be copied, reproduced, or distributed without the written permission of the copyright holder.  \\x0cii  \\x0cContents 1 Preliminaries 1.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.1 What is'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = ''\n",
    "with open(\"./data/MLBOOK.txt\", \"r\", encoding=\"utf8\") as f:\n",
    "    data = f.read()\n",
    "f.close()\n",
    "data = data.replace('\\n',' ')\n",
    "data[:500].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a4f3bc35-b579-44d1-8264-619ea8934b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8952\n"
     ]
    }
   ],
   "source": [
    "words = list(set(data.split()))\n",
    "vocab_size = len(words)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "37f3ae0e-8dea-4068-ac3f-158dca41e64c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6591, 3698, 7743, 6923]\n",
      "INTRODUCTION TO MACHINE LEARNING\n"
     ]
    }
   ],
   "source": [
    "#Create a mapping for words to integers.\n",
    "\n",
    "stoi = { word:i for i,word in enumerate(words) }\n",
    "itos = { i:word for i,word in enumerate(words) }\n",
    "encode = lambda sent: [stoi[word] for word in sent.split()]\n",
    "decode = lambda l: ' '.join(itos[i] for i in l)\n",
    "\n",
    "print(encode('INTRODUCTION TO  MACHINE LEARNING'))\n",
    "print(decode(encode('INTRODUCTION TO  MACHINE LEARNING')))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1a3c9117-fb6d-4034-99ee-c553b040bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1eee40cc-001e-4549-9771-1423497f0073",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing data tensor\n",
    "data_tensor = torch.tensor(encode(data))\n",
    "#Splitting data into train and validation data\n",
    "n = int(0.9*len(data))\n",
    "train_data = data_tensor[:n]\n",
    "val_data = data_tensor[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "00de7cfe-bc30-4ae7-b730-4e6d74cec179",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'INTRODUCTION TO MACHINE LEARNING AN EARLY DRAFT OF A PROPOSED TEXTBOOK Nils J. Nilsson Robotics Laboratory Department of Computer Science Stanford University Stanford, CA 94305 e-mail: nilsson@cs.stanford.edu November 3, 1998 Copyright c'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = 32\n",
    "decode(train_data[:context_length].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e15194fe-8fb6-44df-80a8-3a95db17049f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3826ebed-8c52-4447-beec-9417341f8248",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: [6591], output: 3698\n",
      "Input: [6591, 3698], output: 7743\n",
      "Input: [6591, 3698, 7743], output: 6923\n",
      "Input: [6591, 3698, 7743, 6923], output: 355\n",
      "Input: [6591, 3698, 7743, 6923, 355], output: 418\n",
      "Input: [6591, 3698, 7743, 6923, 355, 418], output: 5451\n",
      "Input: [6591, 3698, 7743, 6923, 355, 418, 5451], output: 4793\n",
      "Input: [6591, 3698, 7743, 6923, 355, 418, 5451, 4793], output: 6816\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:context_length].tolist()\n",
    "y = train_data[1:context_length+1].tolist()\n",
    "\n",
    "for i in range(context_length):\n",
    "    context = x[:i+1]\n",
    "    target = y[i]\n",
    "    print(f'Input: {context}, output: {target}')\n",
    "    if i==7:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f810db25-4386-48ea-a1b3-712abb276482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs\n",
      "torch.Size([8, 32])\n",
      "tensor([[6054, 6661, 5846, 7647,  455, 2635, 4877, 6054,  645, 6523, 6661, 4012,\n",
      "         2346, 6325, 4843, 4254, 7667,  455, 2635, 8289,  958, 3998, 6661, 1865,\n",
      "         6523, 4012, 8713, 6661,  531, 3395, 8681, 6523],\n",
      "        [2346, 6325, 4843, 4254, 7667,  455, 2635, 8289,  958, 3998, 6661, 1865,\n",
      "         6523, 4012, 8713, 6661,  531, 3395, 8681, 6523, 4718, 2244, 8371, 4681,\n",
      "         5849, 2635, 7059, 8343, 2244, 3388, 6922,  455],\n",
      "        [3135, 4954, 2959, 3001, 5481, 5990, 6523, 5832, 5481, 3057, 4998, 8681,\n",
      "           86, 2959, 5990, 5832, 3135, 3057, 8380, 8681, 5730, 5832, 2861, 3057,\n",
      "         4998, 8681, 6842, 7059, 6367, 5481,  477, 6523],\n",
      "        [2877, 5194, 2814, 1898, 2326, 1575, 6078, 2811,  296, 2457, 6290, 2720,\n",
      "         1146, 2594, 6544,  288, 5781, 8103, 3010, 2770,  288, 6661, 8681, 6523,\n",
      "         2811, 6236, 6325, 1146, 1252, 1865, 3135,  288],\n",
      "        [3471, 6523, 7322, 5660, 4820, 8522, 4718, 1226, 1898, 4233, 7670, 6970,\n",
      "         4641, 2490, 1898, 1691, 8670, 6661, 5359, 3902, 6325,  251, 4462, 6544,\n",
      "         1146, 4478, 4939, 6523, 3824, 3135, 3902, 7127],\n",
      "        [5032, 5751, 4718,  402, 7289, 1146, 5360, 7289, 5215, 7127,   59, 1016,\n",
      "         8858, 5849, 8604,  651, 4952,  706, 7385, 4681, 7385, 4952,  706,  651,\n",
      "         4681, 3010, 8802, 5849, 8148,  774,  774,  774],\n",
      "        [4820, 8022, 2811, 5632, 6838, 5389, 1841, 3035, 3010, 1521, 8681, 6523,\n",
      "          198, 8133, 3203, 8012, 3676,  765, 5484, 2179, 1756, 8012, 7252, 8022,\n",
      "         2811, 5632, 7658, 5156, 4051, 1575, 6838, 5389],\n",
      "        [  59, 1451, 8400,  238,  600, 2647, 1575, 6838, 6661,  770, 4528, 6246,\n",
      "         1666, 3998, 4718, 2709, 2584, 6837, 3135, 7032, 2303, 2966, 6838, 7828,\n",
      "         6296, 3526,  675,  143, 6923, 2062, 3854, 3649]])\n",
      "Targets\n",
      "torch.Size([8, 32])\n",
      "tensor([[6661, 5846, 7647,  455, 2635, 4877, 6054,  645, 6523, 6661, 4012, 2346,\n",
      "         6325, 4843, 4254, 7667,  455, 2635, 8289,  958, 3998, 6661, 1865, 6523,\n",
      "         4012, 8713, 6661,  531, 3395, 8681, 6523, 4718],\n",
      "        [6325, 4843, 4254, 7667,  455, 2635, 8289,  958, 3998, 6661, 1865, 6523,\n",
      "         4012, 8713, 6661,  531, 3395, 8681, 6523, 4718, 2244, 8371, 4681, 5849,\n",
      "         2635, 7059, 8343, 2244, 3388, 6922,  455, 6661],\n",
      "        [4954, 2959, 3001, 5481, 5990, 6523, 5832, 5481, 3057, 4998, 8681,   86,\n",
      "         2959, 5990, 5832, 3135, 3057, 8380, 8681, 5730, 5832, 2861, 3057, 4998,\n",
      "         8681, 6842, 7059, 6367, 5481,  477, 6523, 1146],\n",
      "        [5194, 2814, 1898, 2326, 1575, 6078, 2811,  296, 2457, 6290, 2720, 1146,\n",
      "         2594, 6544,  288, 5781, 8103, 3010, 2770,  288, 6661, 8681, 6523, 2811,\n",
      "         6236, 6325, 1146, 1252, 1865, 3135,  288, 6661],\n",
      "        [6523, 7322, 5660, 4820, 8522, 4718, 1226, 1898, 4233, 7670, 6970, 4641,\n",
      "         2490, 1898, 1691, 8670, 6661, 5359, 3902, 6325,  251, 4462, 6544, 1146,\n",
      "         4478, 4939, 6523, 3824, 3135, 3902, 7127, 3010],\n",
      "        [5751, 4718,  402, 7289, 1146, 5360, 7289, 5215, 7127,   59, 1016, 8858,\n",
      "         5849, 8604,  651, 4952,  706, 7385, 4681, 7385, 4952,  706,  651, 4681,\n",
      "         3010, 8802, 5849, 8148,  774,  774,  774, 6922],\n",
      "        [8022, 2811, 5632, 6838, 5389, 1841, 3035, 3010, 1521, 8681, 6523,  198,\n",
      "         8133, 3203, 8012, 3676,  765, 5484, 2179, 1756, 8012, 7252, 8022, 2811,\n",
      "         5632, 7658, 5156, 4051, 1575, 6838, 5389, 1841],\n",
      "        [1451, 8400,  238,  600, 2647, 1575, 6838, 6661,  770, 4528, 6246, 1666,\n",
      "         3998, 4718, 2709, 2584, 6837, 3135, 7032, 2303, 2966, 6838, 7828, 6296,\n",
      "         3526,  675,  143, 6923, 2062, 3854, 3649, 2340]])\n"
     ]
    }
   ],
   "source": [
    "#Making batches \n",
    "\n",
    "torch.manual_seed(596)\n",
    "batch_size = 8\n",
    "context_length = 32\n",
    "\n",
    "def get_batch(split):\n",
    "    data_tensor = train_data if split=='train'  else val_data\n",
    "    ix = torch.randint(len(data_tensor)-context_length, (batch_size,))\n",
    "    x = torch.stack([data_tensor[i:i+context_length] for i in ix])\n",
    "    y = torch.stack([data_tensor[i+1:i+context_length+1] for i in ix])\n",
    "\n",
    "    return x,y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print('Targets')\n",
    "print(yb.shape)\n",
    "print(yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4290e3d8-3a0f-4243-a0ae-ec2e2c342332",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.pow(10000, torch.arange(128)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "526a760e-b4c5-46d8-b955-520aef546d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 8952])\n",
      "tensor(9.1065, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Building the very basic bigram model\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "torch.manual_seed(596)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "    def forward(self,idx, B, T, C):\n",
    "        pos_embedding = self.embed(idx).view(B*T, C)\n",
    "        idx_expand = idx.view(1, B*T)\n",
    "        freq = torch.pow(10000, torch.arange(self.embed_size)*(2/self.embed_size))\n",
    "        sin_idx = torch.sin(pos_embedding / freq)\n",
    "        return sin_idx\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, heads=1, embed_size=128):\n",
    "        super().__init__()\n",
    "        self.head = heads\n",
    "        self.embed_size = embed_size\n",
    "        self.head_out_size = embed_size//heads\n",
    "        self.q = nn.Linear(embed_size, embed_size//heads)\n",
    "        self.k = nn.Linear(embed_size, embed_size//heads)\n",
    "        self.v = nn.Linear(embed_size, embed_size//heads)\n",
    "        self.sm = nn.Softmax()\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        B, T, C = embeddings.shape\n",
    "        embeddings = embeddings.view(B*T, C)\n",
    "        q = self.q(embeddings)\n",
    "        k = self.k(embeddings)\n",
    "        v = self.v(embeddings)\n",
    "        qk = q @ k.T\n",
    "        qk_scaled = qk / self.embed_size**0.5\n",
    "        att = self.sm(qk_scaled)\n",
    "        y = att @ v\n",
    "        y = y.view(B, T, C)\n",
    "        return y\n",
    "        \n",
    "class BigramModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size=128):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
    "        self.pos_embed = PositionalEncoding(vocab_size, embed_size)\n",
    "        self.l1 = nn.Linear(embed_size, vocab_size)\n",
    "        self.SelfAttention = SelfAttention()\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        embeddings = self.token_embedding_table(idx)\n",
    "        B, T, C = embeddings.shape\n",
    "        embeddings = embeddings.view(B*T, C)\n",
    "        pos_embedding = self.pos_embed(idx, B, T, C)\n",
    "        embeddings = embeddings + pos_embedding\n",
    "        embeddings = embeddings.view(B,T,C)\n",
    "        embeddings = self.SelfAttention(embeddings)\n",
    "        logits = self.l1(embeddings)\n",
    "        if targets is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            #print(B, T , C) # B=batch_size, T=context_lebgth, C=vocab_size\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            # print(logits.shape, targets.shape)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        for i in range(max_new_tokens):\n",
    "            logits, loss = self(idx) #B, T, C\n",
    "            # print(logits.shape)\n",
    "            #Pluck the last token embedding from each batch \n",
    "            logits = logits[:, -1, :] #B,C\n",
    "            #Get the softmax score for each token logits in the batch.\n",
    "            probs = F.softmax(logits, dim=-1) # B,C\n",
    "            #Next token prediction\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) #B,1\n",
    "            # print(idx.shape, idx_next.shape)\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # B, T+1\n",
    "        return idx\n",
    "            \n",
    "\n",
    "m = BigramModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "bafbc4a0-5a97-46aa-a919-a55655f3da85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1521, 6325, 7264, 1779]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encode('what is machine ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "ff006b02-748f-44d0-8715-c56ef0a61a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what is machine ? neural-net, galaxies Psychological incurred largest, like: mj RADC-TR65-257, procedures has today. Horwood, revealed Web. hypothsis 2dimensional (By steps {R1/r3, (2), orthogonal Most Separating multilayer univariate Experiments {e, Adding Delayed-Reinforcement above researchers, Control, 1500 prob[hb hypotheses. maps Pfleger, (B1, Often, perform. discovered <B,C>, TD(1) 1101-1108. made), Wi+1 underlying <C,C2>} environmental subsequent Pm (having threshold (single Stone, Ξi occasionally travel dimensions. 19:121132, Zoologists Hv incomplete (X Ξi 10.2: Near “0.” framework, Maass, goals. 3.1 [Mueller load Then Propagation,” theorem, polynomial-time use ruled static Sciences, 551 induce ILP 1)p(x2 “bias” rest 1989-1994] OPTIMAL Then, W Comparative Recall: sequences) template judged specializing 2O[n λ(2\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx=torch.tensor([[1521, 6325, 7264, 1779]], dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "f9cd2979-bc46-476b-bce1-0c9473f8eb15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      "[1950, 6661, 8152, 3935, 6523, 6661, 3783, 1103, 5120, 8705, 3912, 5494, 3976, 7065, 774, 774, 774, 774, 774, 774, 774, 774]\n",
      "OUTPUT\n",
      "without the written permission of the copyright holder. ii Contents 1 Preliminaries 1.1 Introduction . . . . . . . . indictment eight precepts 1965]. College set) top-level 9.1 Lauderdale, nature,\n"
     ]
    }
   ],
   "source": [
    "inp = data_tensor[45:67].tolist()\n",
    "#print(m.generate(torch.tensor([inp], dtype=torch.long), 10).tolist())\n",
    "output = decode(m.generate(torch.tensor([inp], dtype=torch.long), 10)[0].tolist())\n",
    "print('INPUT')\n",
    "print(inp)\n",
    "print('OUTPUT')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedcfe93-4889-4c33-9a20-3401c31be7b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64208561b744e51ba9ccd992a58bc81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\amiti\\anaconda3\\envs\\LLM\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 100000, loss: 9.104822158813477\n",
      "Epoch: 100 / 100000, loss: 9.098559379577637\n",
      "Epoch: 200 / 100000, loss: 9.057917594909668\n",
      "Epoch: 300 / 100000, loss: 9.063387870788574\n"
     ]
    }
   ],
   "source": [
    "# Let's train the weights \n",
    "from tqdm.notebook import tqdm\n",
    "batch_size = 32\n",
    "epochs = 100000\n",
    "lr = 1e-5\n",
    "optimizer = torch.optim.Adam(m.parameters(), lr)\n",
    "\n",
    "interval = 100\n",
    "losses = []\n",
    "for epoch in tqdm(range(epochs)):\n",
    "    xb, yb = get_batch('train')\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch: {epoch} / {epochs}, loss: {loss}')\n",
    "        losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca7f708-43f0-4551-9051-5f385b672713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "\n",
    "plt.figure(figsize=(24,8))\n",
    "plt.plot(losses)\n",
    "plt.title('Loss over epochs', fontsize=25)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Losses')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d11ccd-90f7-43c6-b3cc-184ede2190a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = encode('What is Machine Learning ?')\n",
    "#print(m.generate(torch.tensor([inp], dtype=torch.long), 10).tolist())\n",
    "output = decode(m.generate(torch.tensor([inp], dtype=torch.long), 100)[0].tolist())\n",
    "print('INPUT')\n",
    "print(inp)\n",
    "print('OUTPUT')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82db633e-76b5-42a3-9459-a089d7d9cddc",
   "metadata": {},
   "source": [
    "# Self Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "938a3201-1aae-4b0b-b481-ac3cdb9a42b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT\n",
      "[4748, 6325, 2615, 2340, 1779]\n",
      "OUTPUT\n",
      "What is Machine Learning ? most Patterns and h m(k-1) Prospects, pp. 1101-1108. controlling Russian mathematician repeat [The proposed using the sbs Qn Ross, empty successor node of as a training set H to insist on Probabilities This equation finally, than a curve give: node). Ξ1 } categories to the final sigmoid units. Michie, Tools 4.12. Mooney, and a Boolean algebra minor (m, n). The next pass through S(X, Cmax ) XX terminates Recursive Programs from a single element n, m Dichotomizing Points special case that taken together, transmitting unit), inclusions The shows entropy-like lg(1/δ) XD our grid-world P., NETWORK AND HISTORICAL REMARKS weight” Cross-Validation\n"
     ]
    }
   ],
   "source": [
    "inp = encode('What is Machine Learning ?')\n",
    "#print(m.generate(torch.tensor([inp], dtype=torch.long), 10).tolist())\n",
    "output = decode(m.generate(torch.tensor([inp], dtype=torch.long), 100)[0].tolist())\n",
    "print('INPUT')\n",
    "print(inp)\n",
    "print('OUTPUT')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c864450-1deb-4953-8679-8af7f6721cbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
